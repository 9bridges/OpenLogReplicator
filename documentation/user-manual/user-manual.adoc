= User Manual
:author: Adam Leszczyński <aleszczynski@bersler.com>
:revnumber: 1.2.0
:revdate: 2022-04-20
:imagesdir: ./images
:url-github-docker: https://github.com/bersler/OpenLogReplicator-Docker
:url-github-librdkafka: https://github.com/edenhill/librdkafka
:toc: preamble

[frame="none",grid="none"]
|====
a|[.small]#Autor: {author}, version: {revnumber}, date: {revdate}#
|====

This document is the main user manual for OpenLogReplicator.

== Quick start

For a quick start with OpenLogReplicator, please refer to the GitHub repository containing tutorials. Link to the repository will soon available.

=== Running in Docker container

Development and testing of OpenLogReplicator is done using Docker containers.
It is the best environment to run and test the program.
For submitting bugs and feature requests, please use the {url-github-docker}[GitHub] repository.

== Architecture

OpenLogReplicator is written in C++.
It uses Oracle client libraries to connect to the database.

Transactions are directly decoded from redo log files.
If possible, online redo logs are parsed, but if it is not possible –- archived redo logs are parsed instead.
It is crucial that the process has physical access to files and the files are not deleted before they are processed.

HINT: Refer to Oracle Database manual for defining retention period for archived redo logs.

The architecture of the program is multithreading.
The following threads are used:

1. Main thread: used for program startup and shutdown

2. N Reader threads: used to read redo log files from disk -- there is one thread per redo log file (group).
Most of the time just one thread is active, the other is sleeping and not utilizing the CPU.
Only during short time when the log switch is performed and the new redo log file is created, there may be more than one active thread.
The reader thread uses buffering to read data from disk.
The size of the buffer is configurable.

3. Checkpoint thread: used to write checkpoint file to disk.
This is done every 10 seconds.
The checkpoint file contains information about the last processed redo log file and the last processed block in this file.
This information is used to restart the program from the last processed position.
Writing the checkpoint file together with the schema is not blocking the main parser thread.

4. Parser thread: used to parse redo log files.
Parsing is done serially using just one thread.
In case of DDL changes, the schema is updated and the checkpoint file is written to disk.
The parser thread is also responsible for sending transactions to output buffer in a format which is configured for output (JSON or Protobuf).

5. Writer thread: responsible for sending the transactions from the output buffer to the output.
There might be more threads used to control proper communication with the output depending on the type of the connector and threads used by the external libraries.

NOTE: The only CPU-intensive thread is the parser thread.
Other threads perform operations which are not CPU intensive, however, might be I/O intensive.

To prevent multiple instances of OpenLogReplicator running simultaneously and overwriting checkpoint files ane another, the program creates a lock on the config file after startup.
If another instance is already running and the lock can't be created, the program exits with an error.

=== Memory Allocation

For most data structures, including disk buffers, output buffers and copy of the redo log vectors the program uses one big pool of memory, which is fully controlled by 2 parameters: `memory-min-mb` and `memory-max-mb`.
Those parameters allow to fully control the memory usage of the program.
Apart from main memory structures, the program uses dynamic memory allocation from heap for storing metadata (table names, types, columns names, etc.).
Currently also LOB data is stored in dynamic memory, but is planned to be moved to main memory buffers.

=== Database redo logs

All changes in the database -- results of DML commands like INSERT, UPDATE, DELETE -- are written to database redo logs.
The redo log files contain information about what has been changed.
It would not contain information about metadata for every transaction -- like number of columns in the table, names, types.
Such information should be cached in memory and updated when DDL operations are performed.

To operate properly during startup, OpenLogReplicator needs to collect information about schema during the initial run.
It would read database system tables to collect all data about schema.
This information is stored in a file and used during the following runs.

IMPORTANT: OpenLogReplicator does *NOT* perform the task of *initial data load*.
It never connects to the source data and runs SELECT queries.
It only reads redo log files and sends information about changes to the output.
For a complete replication solution, you need to use another tool like ETL or restore a database from backup.

CAUTION: After start, all redo log files must be available in their entirely.
Not a single redo log files block may be missing.
If a file or even one block is missing, replication needs to be re-initiated.
Schema information should be collected from the beginning, and replication re-initiated.

=== Transaction processing

Database redo log files contain both committed and rolled-back transactions.
DML operations are written to redo log files as they are executed.
All operations are flushed when the COMMIT record appears.
This is a guarantee point for the database client that all changes have been accepted by the database and are durable and visible to other clients (ACID properties).

NOTE: DMLs from different transactions are interleaved in the redo log files.

NOTE: The redo log files contain also information about transactions that eventually were rolled back.
Or are partially rolled back.

The task of OpenLogReplicator is to sort DML commands and send them to output in proper order:

1. All rolled back transactions are ignored

2. All partially rolled back DML commands are ignored

3. All committed transactions are sent to output as soon as the commit record appears

4. Transactions are sorted by commit time

5. Transactions are sent to output without any interleaving

A transaction that is sent to output may be in one message or may be divided into multiple messages -- one message for the beginning of the transaction, one for commit and one for each DML command.

The number of details in the message is configurable.

*TODO:* image with interleaved transactions

=== Transaction caching & restart

All transactions which are active (started) are cached in memory.
They are cached so long, how long the transaction is open.
After the transaction is committed and data processed, memory is released.
If the transaction is big –- the program would need more memory.
OpenLogReplicator never writes any additional files to disk beside of checkpoint and schema file.

CAUTION: When OpenLogReplicator is restarted –- it would need to go back to the start of the oldest unprocessed transaction location and start reading database redo logs from this position.
This may mean going back a long time and process again the same redo log files which have already been processed before the restart appeared.
Transactions which were sent to output would not be sent again.
This operation may be time and resource consuming.
It is recommended to restart OpenLogReplicator only when it is necessary.

TIP: Configure database redo log retention strategy to leave enough redo log files to be able to restart OpenLogReplicator.

*TODO:* image describing restart

=== Topology

There are 2 possible scenarios of running OpenLogReplicator: on the database host and on another host.

==== Running on the database host

This is the easiest and most efficient solution.
But it is not recommended for production systems, as the database performance might be affected when CPU or memory is saturated.

CAUTION: OpenLogReplicator may be using extensive memory and CPU.
Please make sure that there are enough resources for the database to work properly.
OpenLogReplicator should use only part of the memory, so that there is memory available for the database.

*TODO* image

==== Running on another host

This is the recommended solution.
For this scenario, you must make sure that the redo log files are possible to read.
This may be achieved by:

* mounting read only remote filesystem, (for example, using SSHFS),

* reading from SRDF copy,

* reading from standby database,

* reading just archived redo logs copied by batch file;

.Remote access to redo log files
image:architecture-sshfs.png[Remote access to redo log files,,,]

OpenLogReplicator by default would read online redo logs and process transactions as soon as they are committed, and this information is written to redo log.
But it can also read just archived redo logs – in this scenario transactions would be processed when log switch is performed and redo log is archived.

== Output Format

=== JSON format

*TODO:* write

=== Protocol buffer format

*TODO:* write

== Output target

=== Kafka target

OpenLogReplicator is a standalone program which connects to Kafka and sends messages to it.
The connection parameters are fully controlled from the program parameters.

NOTE: The Kafka target connector is not a Kafka Connect module.

==== Build instructions

By default, OpenLogReplicator does not have the Kafka writer modules compiled in.
The Kafka target module needs to be compiled and liked with the code.
For {url-github-docker}[Docker images] use the parameter `--build-arg WITHKAFKA=1`.
The Kafka module client is written in C/C++ and uses no Java code or runtime.

==== Limitations

CAUTION: OpenLogReplicator uses the {url-github-librdkafka}[librdkafka] library to connect to Kafka.
The library has a limitation of 1.000.000.000 bytes for maximum message size.
OpenLogReplicator has no limit for message size and can process rows containing multiple LOB columns which are up to 4GB in size.
A message can theoretically contain multiple LOB columns, which would exceed the maximum message size for the Kafka client library.

==== Idempotent Producer

OpenLogReplicator can act as an idempotent producer.
This is default behavior and is controlled by the `enable-idempotence` parameter.

==== Performance

For performance reasons, OpenLogReplicator sends the message asynchronously and does not wait for the confirmation from Kafka.
The number of messages sent simultaneously to Kafka is controlled by the `max-messages` parameter.

==== Checkpointing

TODO: write

=== File target

File target is the simplest target.
It writes the output to a file.
There are no limitations for the file size or the message size.

TIP: For reproduction cases, whenever possible, use the file target.
Such reproduction requires no setup of the Kafka cluster and is easier to set up.

==== Checkpointing

TODO: write

=== Network target

The network target is the most sophisticated module and allows to send the output to virtually any type of target.
The receiver of the messages can be written in any language (C, C++, GO, Java, Rust, etc.) and can be running on any platform.

The intention of the network module is to allow maximum integration, while keeping the code simple and easy to maintain.

There are no limitations for the message size for the network module.

The network module is flexible when it comes to communication protocol.
Currently, 2 protocols are supported: plain TCP/IP and ZeroMQ, but other protocols can be added easily.

==== Checkpointing

For the network communication protocol, the receiver of the messages controls the position of the checkpoint.
This means that the receiver is responsible for saving the checkpoint and for sending the checkpoint to the sender.
The receiver (Target) informs the sender (OpenLogReplicator) that certain transactions have been accepted op to the defined SCN position.
In case of connection failure, the sender (OpenLogReplicator) would start from the last checkpoint position provided by the target.

IMPORTANT: This allows creating an HA configuration!

IMPORTANT: It is not possible to retry the

==== Architecture

TODO: describe the principles of the network target

==== Communication protocol

TODO: write

== Supported Features

=== DML operations

The following operations are supported:

- INSERT operation (including multiple row INSERT, but not direct path -– INSERT `/\*+append*/`);
- UPDATE operation;
- DELETE operation (including multiple row DELETE).

=== Table parameters

Tables with the following parameters are supported:

- null/not null columns;
- invisible columns;
- columns with null and default values;
- up to 1000 columns (database maximum till 21c);
- row chaining/migration;
- partial rollbacks (rollback to savepoint);
- partitioned tables;
- tables with rowdependencies;
- BASICFILE LOBs.

Transactions that are rolled back are not processed.

Transactions are processed as soon as they are committed (not earlier).

Every transaction is tracked since every transaction can eventually contain a DML operation of a tracked table.

=== Column types

List of supported column types (with internal Oracle codes):

- 1 –- varchar2/nvarchar2 (including out of row stored as LOB), supported <<supported-character-sets,character sets>>;
- 2 –- number/float;
- 12 –- date;
- 23 –- raw;
- 96 –- char/nchar (list of supported character sets);
- 100 –- binary_float;
- 101 –- binary_double;
- 112 -- clob;
- 113 -- blob;
- 180 –- timestamp;
- 181 –- timestamp with time zone;
- 182 -- interval year to month;
- 183 -- interval day to second;
- 208 -- urawid.

If a table contains column types which are not supported –- `“?”` value is presented in output data.

=== DDL operations

Changes in the schema are supported.

TODO: more

=== Supported Character Sets [[supported-character-sets]]

OpenLogReplicator supports many character sets which can be used in the source Oracle database.

All character fields are read from the source database in respect to the source character set.
The output message always uses Unicode as character encoding and UTF-8 format.
OpenLogReplicator does not perform any left-to-right Unicode character replacements.

For test purposes, the character set conversion can be disabled.
Please check the xref:../reference-manual/reference-manual.adoc#char[char] parameter for details.

Full list of supported character sets is: *AL16UTF16*, *AL32UTF8*, AR8ADOS710, AR8ADOS710T, AR8ADOS720, AR8ADOS720T, AR8APTEC715, AR8APTEC715T, AR8ARABICMACS, AR8ASMO708PLUS, AR8ASMO8X, AR8HPARABIC8T, AR8ISO8859P6, AR8MSWIN1256, AR8MUSSAD768, AR8MUSSAD768T, AR8NAFITHA711, AR8NAFITHA711T, AR8NAFITHA721, AR8NAFITHA721T, AR8SAKHR706, AR8SAKHR707, AR8SAKHR707T, AZ8ISO8859P9E, BG8MSWIN, BG8PC437S, BLT8CP921, BLT8ISO8859P13, BLT8MSWIN1257, BLT8PC775, BN8BSCII, CDN8PC863, CEL8ISO8859P14, CL8ISO8859P5, CL8ISOIR111, CL8KOI8R, CL8KOI8U, CL8MACCYRILLICS, CL8MSWIN1251, D7DEC, D7SIEMENS9780X, DK7SIEMENS9780X, E7DEC, E7SIEMENS9780X, EE8ISO8859P2, EE8MACCES, EE8MACCROATIANS, EE8MSWIN1250, EE8PC852, EEC8EUROASCI, EEC8EUROPA3, EL8DEC, EL8ISO8859P7, EL8MACGREEKS, EL8MSWIN1253, EL8PC437S, EL8PC737, EL8PC851, EL8PC869, ET8MSWIN923, HU8ABMOD, HU8CWI2, I7DEC, I7SIEMENS9780X, IN8ISCII, IS8MACICELANDICS, IS8PC861, IW8ISO8859P8, IW8MACHEBREWS, IW8MSWIN1255, IW8PC1507, JA16EUC, JA16EUCTILDE, JA16EUCYEN, JA16SJIS, JA16SJISTILDE, JA16SJISYEN, JA16VMS, KO16KSC5601, KO16KSCCS, KO16MSWIN949, LA8ISO6937, LA8PASSPORT, LT8MSWIN921, LT8PC772, LT8PC774, LV8PC1117, LV8PC8LR, LV8RST104090, N7SIEMENS9780X, N8PC865, NDK7DEC, NE8ISO8859P10, NEE8ISO8859P4, RU8BESTA, RU8PC855, RU8PC866, S7DEC, S7SIEMENS9780X, SE8ISO8859P3, SF7ASCII, SF7DEC, TH8MACTHAIS, TH8TISASCII, TIMESTEN8, TR8DEC, TR8MACTURKISHS, TR8MSWIN1254, TR8PC857, US7ASCII, US8PC437, *UTF8*, VN8MSWIN1258, VN8VN3, WE8DEC, WE8DG, WE8HP, WE8ISO8859P1, WE8ISO8859P15, WE8ISO8859P9, WE8MACROMAN8S, WE8MSWIN1252, WE8NCR4970, WE8NEXTSTEP, WE8PC850, WE8PC858, WE8PC860, WE8ROMAN8, ZHS16CGB231280, ZHS16GBK, ZHS32GB18030, ZHT16BIG5, ZHT16CCDC, ZHT16HKSCS, ZHT16HKSCS31, ZHT16MSWIN950, ZHT32EUC, ZHT32TRIS.

The target character set is always Unicode and UTF-8 format.

== Replication

=== Selection

=== Setup



== Startup

=== Position by scn

=== By time

=== By sequence



== Checkpointing

Checkpointing is a way to save the current position in the redo log.
The structure of redo log files is organized as LWN's (Log Writer Number).
Each LWN has a sequence of blocks which is a unit of redo log records and is processed as a whole.
Because of this, the position is advanced only after the whole LWN is processed.

=== Checkpoint messages

After every LWN is processed, OpenLogReplicator emits a special record called checkpoint record.
The checkpoint record contains the current position in the redo log.

An example of checkpoint records:

 "payload":[{"op":"chkpt","seq":1763,"offset":15872}]

Checkpoint records can be used as a heartbeat signal, and are enabled by default.
Sending of the checkpoint record can be disabled by setting of the `"flags":4096` parameter of the `reader`.

The record contains information about the current sequence of the redo log files (`seq`) and the offset in the current file (`offset`).
Sometimes it also contains information about the fact that the redo log file has been switched (`"redo":true`).

Using the checkpoint records is the best way of verifying that the replication is working properly.
Oracle database writes LWNs every few seconds, so the checkpoint records are emitted every few seconds as well.
In case of any problems, the checkpoint records would stop being emitted.

=== Checkpoint SCN

The SCN value present in the checkpoint record is used as a checkpoint SCN.
When OpenLogReplicator is restarted, it starts from the last checkpoint SCN.
Transactions which have commit SCN lower than the checkpoint SCN are not processed.

== Output

=== Data format

=== DDL and schema changes

=== JSON messages

=== Protobuf messages



== Running

=== Docker



== Advanced Topics

The following chapter describes some advanced topics, which would normaly not be used by a typical user.

=== Schema changes

OpenLogReplicator can handle schema changes.

To work properly, an initial consistent image of the schema needs to be created.
This is done using the `gencfg.sql` script or during the first run of the program.

The image of database system tables is read just for a selected list of schemas.
All objects owned by any of the database users from the list would be tracked in the future.
Also objects created, moved to bin, recovered, etc.

IMPORTANT: In the case of adding new schema to the replication list -- the process needs to be restarted, and the schema needs to be recreated.

While OpenLogReplicator is running and a new object is created which would match the filter, the schema is automatically updated.
There is no need to restart the process.

CAUTION: It is not a good choice to select too many schemas for tracking, as this can impact on performance.

All DDL operations which modify data structures are tracked: adding, removing columns, changing names, extending type length, etc.

For partitioned tables, the schema is automatically updated when a new partition or subpartition is created.
There is no need for user intervention or program restart.
In fact, restarting the program would have no impact on the schema, because it would not cause a new schema image to be created.

=== Offline Reader

For cases where there is no physical way to connect the database, OpenLogReplictor offers
an offline reader mode.

In such scenario, a replication may be fully functional, even though there is no network connection to the database.
This also implies that there is no need to create a user in the database.

The offline reader mode is activated by setting a parameter `"type": "offline"` in the `reader` section.

To start work with the offline reader, a schema file is needed.
The schema file can be created using the `gencfg.sql` script or during the first run of the program when running with `online` reader.

CAUTION: The schema file contains also list locations of redo log files.
Those parameters are static and are not tracked.
Thus, if they change, the schema file needs to be edited manually (this is a json file), or the schema needs to be recreated.

==== Starting as Offline Reader

To start replication with `online` reader, first define the type of the reader as `online`, provide credentials for the database connection and start the program.

Once the program is running, it would create a schema file -- which is visible in the `checkpoint` folder.

Stop OpenLogReplicator, by simply pressing `Ctrl+C` or killing the process.

Next, edit configuration file and change the type of the reader to `offline`, remove credentials for database connection and run OpenLogReplicator.

==== Manually creating schema file

To create a schema file manually, the user needs to run the `gencfg.sql` script.
The script is located in the `scripts` folder of the distribution.

Before running the script, it must be edited and the following parameters needs to be provided:

- `v_NAME` -- the logical name used in the schema file;
- `v_USERNAME_LIST` -- list of usernames to be tracked;
- `v_SCN` -- the starting SCN for replication.

Save the results of the script to a file with name `checkpoint/<name>-chkpt-<scn>.json` where `<name>` is the logical name of the database and `<scn>` is the starting SCN for replication.
Read the `resetlogs` and `activation` parameters from the first line of the output of the script.
Use the read values during the next steps.
Create additional file named `checkpoint/<name>-chkpt.json` with the following content:

 {"database":"<name>","scn":<scn>,"resetlogs":<resetlogs>,"activation":<activation>}

Provide the values for parameters `<name>`, `<scn>`, `<resetlogs>` and `<activation>` from previous step.

After the files are created, the program can be started with the `offline` reader type.

=== DataGuard

For a database which is using DataGuard, the program can be configured to read redo logs from the standby database.

During the initial startup, the program needs to be started on the primary database.

Only physical DataGuard is supported.

Whenever a switch occurs, the program would automatically switch to the new primary/standby database redo log files.

There is no need to add extra options for the DataGuard configuration.
OpenLogReplicator would automatically detect the DataGuard configuration and start reading appropriate redo log files depending on the current role of the database that it is connecting to.

=== High Availability

OpenLogReplicator is designed to work in HA environment, especially when Network Mode is being used.
The program is not stateless, but keeps all checkpoint data organized.
The current implementation stores the files in a directory, but future versions might store this data, for example, in Redis or other external storage.

The best configuration for HA is Stream Mode.

=== Schemaless Mode

Sometimes the schema is not available, but in spite of that, the user still wishes to decode the data from redo log.
In this case, the user can use the schemaless mode.
The schemaless mode is activated by setting parameter `"flags": 2`.

IMPORTANT: Since table names are not available, table filtering is not used.
The output would contain all tables, even if they are not in the filter.

After activation, OpenLogReplicator would ignore the lack of schema and lack of information about column data types.
The data would contain column values in the form of hex strings, since the information about type is not available.
Column names and table names are missing, since the information about schema is not available.

Example output for schemaless mode:

 "payload":[{"op":"c","schema":{"table":"OBJ_87705"},"after":{"COL_0":"53544f50","COL_1":"787708010d2339"}}]

TIP: Schemaless mode is extremely useful when the user just has the redo log file and nothing more but wishes to decode the data.
It may also be useful to use the adaptive schema mode together with schemaless mode.

IMPORTANT: The schemaless mode is not intended for production use.
It is intended for debugging and analysis of the data in the redo log.

=== Adaptive Schema Mode

Adaptive schema mode is a special debugging mode which is used to analyze the data in the redo log even when the schema file is missing.
The adaptive schema mode is activated by setting parameter `"flags": 4`.
This mode works best when used together with schemaless mode.

After enabling first OpenLogReplicator would try to use default schema for system tables.
A default schema would be schema from the source database but created with no table selection.
The default location for default schema file `base-<database version>.json`.

CAUTION: The default schema is not distributed with OpenLogReplicator.
The user needs to create the default schema file manually, by Running OpenLogReplictor with an empty database and copying the schema file from the checkpoint directory.

OpenLogReplicator would act in hybrid mode:

- when a new schema is created: the schema would be extended by the new objects -- for such tables, the schema information would be present in the output;
- for existing tables: the schema information would be missing in the output -- output would be in schemaless mode.

IMPORTANT: The adaptive schema mode is not intended for production use.
It is intended for debugging and analysis of the data in the redo log.

CAUTION: This mode is less restrictive when it comes to schema changes.
In cases when OpenLogReplicator would normally stop because of schema change, it would continue in the adaptive schema mode.
